{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b6ee5d",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f46fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: \n",
      "at eight oclock on thursday morning arthur didnt feel very good\n",
      "the website  is a url that should be removed\n",
      "as well as html tags like  or  and punctuation like  or  or \n",
      "\n",
      "Filtered Tokens: ['eight', 'oclock', 'thursday', 'morning', 'arthur', 'didnt', 'feel', 'good', 'website', 'url', 'removed', 'well', 'html', 'tags', 'like', 'punctuation', 'like']\n",
      "Stemmed Tokens: ['eight', 'oclock', 'thursday', 'morn', 'arthur', 'didnt', 'feel', 'good', 'websit', 'url', 'remov', 'well', 'html', 'tag', 'like', 'punctuat', 'like']\n",
      "Lemmatized Tokens: ['eight', 'oclock', 'thursday', 'morning', 'arthur', 'didnt', 'feel', 'good', 'website', 'url', 'removed', 'well', 'html', 'tag', 'like', 'punctuation', 'like']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "At eight o'clock on Thursday morning, Arthur didn't feel very good.\n",
    "The website https://example.com is a URL that should be removed,\n",
    "as well as HTML tags like <br> or <p>, and punctuation like ! or # or $.\n",
    "\"\"\"\n",
    "\n",
    "# Lowercase the text\n",
    "text = text.lower()\n",
    "\n",
    "# Remove URLs\n",
    "text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "# Remove HTML tags\n",
    "text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# Remove punctuation\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "text = text.translate(punctuation_table)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Output the results\n",
    "print(\"Cleaned Text:\", text)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3bf54",
   "metadata": {},
   "source": [
    "Text Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db162a",
   "metadata": {},
   "source": [
    "1. Bag-of-Words (BoW)\n",
    "Bag-of-Words is a simple yet effective method of converting text to numerical representation. It involves two steps: first, creating a vocabulary of all the unique words in the text, and then, measuring the presence of each word in your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document' 'exists' 'first' 'four' 'is' 'longer' 'made' 'number' 'of'\n",
      " 'second' 'text' 'the' 'this' 'three']\n",
      "[[1 0 1 0 0 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 1 1 1 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 1 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus of text\n",
    "corpus = [\n",
    "    'Text of the first document.',\n",
    "    'Text of the second document made longer.',\n",
    "    'Number three exists.',\n",
    "    'This is number four.',\n",
    "]\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to an array for easy viewing\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Output the feature names and the BoW array\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b336d1d",
   "metadata": {},
   "source": [
    "2. TF-IDF\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It increases proportionally with the number of times a word appears in the document but is offset by the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94722bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document' 'exists' 'first' 'four' 'is' 'longer' 'made' 'number' 'of'\n",
      " 'second' 'text' 'the' 'this' 'three']\n",
      "[[0.4222466  0.         0.53556627 0.         0.         0.\n",
      "  0.         0.         0.4222466  0.         0.4222466  0.4222466\n",
      "  0.         0.        ]\n",
      " [0.3365971  0.         0.         0.         0.         0.42693074\n",
      "  0.42693074 0.         0.3365971  0.42693074 0.3365971  0.3365971\n",
      "  0.         0.        ]\n",
      " [0.         0.61761437 0.         0.         0.         0.\n",
      "  0.         0.48693426 0.         0.         0.         0.\n",
      "  0.         0.61761437]\n",
      " [0.         0.         0.         0.52547275 0.52547275 0.\n",
      "  0.         0.41428875 0.         0.         0.         0.\n",
      "  0.52547275 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to an array for easy viewing\n",
    "X_tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Output the feature names and the TF-IDF array\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(X_tfidf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09b5a7",
   "metadata": {},
   "source": [
    "3. n-gram\n",
    "An n-gram model considers a sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb10c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document' 'document made' 'exists' 'first' 'first document' 'four' 'is'\n",
      " 'is number' 'longer' 'made' 'made longer' 'number' 'number four'\n",
      " 'number three' 'of' 'of the' 'second' 'second document' 'text' 'text of'\n",
      " 'the' 'the first' 'the second' 'this' 'this is' 'three' 'three exists']\n",
      "[[1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer with n-gram\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Here, we look at unigrams and bigrams\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_ngram = ngram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Output the feature names and the n-gram array\n",
    "print(ngram_vectorizer.get_feature_names_out())\n",
    "print(X_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bd49b",
   "metadata": {},
   "source": [
    "4. Word2Vec\n",
    "Word2Vec is a two-layer neural network that processes text by \"vectorizing\" words. Its input is a text corpus, and its output is a set of vectors: feature vectors for words in that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71807acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03\n",
      "  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03\n",
      "  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03\n",
      " -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03\n",
      "  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03\n",
      " -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03\n",
      "  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03\n",
      " -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03\n",
      "  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03\n",
      " -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04\n",
      " -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03\n",
      " -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03\n",
      " -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03\n",
      "  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03\n",
      " -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03\n",
      " -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03\n",
      " -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03\n",
      " -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03\n",
      "  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05\n",
      "  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04\n",
      " -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03\n",
      " -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03\n",
      " -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03\n",
      "  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04\n",
      "  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "#sg = 0, COB model, sg = 1, skip-gram model\n",
    "word2vec_model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=2,sg = 1)\n",
    "\n",
    "# Get the vector for a word\n",
    "word_vector = word2vec_model.wv['document']\n",
    "\n",
    "# Output the vector for the word 'document'\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fd216",
   "metadata": {},
   "source": [
    "5. GloVe\n",
    "Global Vectors for Word Representation (GloVe) is an unsupervised learning algorithm for obtaining vector representations for words. The model is an extension to Word2Vec and is based on matrix factorization techniques on the word-context matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a51a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7285e-01 -9.6449e-02  4.1131e-01  3.7925e-01  8.9352e-01  4.5227e-01\n",
      "  1.9478e-01 -3.6985e-01  5.9704e-01  1.3387e-01  4.2878e-01 -2.8012e-01\n",
      "  2.0141e-01 -1.9995e-02 -6.2983e-02  7.1399e-01  8.9025e-01 -3.1009e-01\n",
      " -1.9911e-01 -4.6591e-01 -8.8145e-01 -5.4318e-01 -5.2839e-01  7.0794e-02\n",
      " -3.1042e-01 -9.8677e-01  1.0283e-01  1.6911e-01 -4.4878e-01  1.6171e-01\n",
      "  3.9394e-01  1.2655e-01 -1.2540e-01 -6.6462e-02 -1.2977e-01 -3.9406e-02\n",
      "  4.4811e-02 -4.2534e-01  2.6742e-02 -3.8609e-01 -8.4547e-01 -6.4412e-02\n",
      "  6.8974e-01  2.4521e-01 -7.3434e-01 -7.7389e-01 -1.5336e-01 -2.9057e-01\n",
      " -6.8358e-01 -3.8785e-01  1.2230e+00  1.7723e-01  1.6004e-01  8.3723e-01\n",
      " -3.1238e-01 -1.3138e+00 -2.6000e-01 -4.8754e-01  1.6751e+00  1.7320e-01\n",
      " -2.9494e-01  1.6038e-01 -5.3087e-01 -9.0950e-01  6.7436e-01 -5.2625e-01\n",
      " -3.0406e-01  8.5552e-01 -2.6879e-01 -9.0492e-01  3.0380e-01  2.0591e-01\n",
      "  3.3439e-01 -6.2308e-01  6.4306e-02  2.2179e-01 -9.2076e-02  2.1894e-01\n",
      " -1.4015e+00 -4.4588e-02  2.6263e-01  1.5343e-01 -8.8158e-04 -2.2226e-02\n",
      " -1.3228e+00 -6.3649e-02  9.7797e-01 -8.1209e-01  1.5083e-02  2.4391e-01\n",
      " -1.9343e-01 -1.7141e-01 -1.1954e-01  1.3623e-01  3.4787e-01 -5.0286e-02\n",
      " -1.8547e-01 -7.1763e-01  1.0898e-01  1.1472e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# You need to downloaded the GloVe pre-trained vectors and unzipped them first\n",
    "# Due to the size of the pre-trained vectors, please complete this part after class\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted GloVe vectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Get the vector for a word\n",
    "word_vector = glove_model['document']\n",
    "\n",
    "# Output the vector for the word 'document'\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bf586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
